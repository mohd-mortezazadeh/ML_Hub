{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pydevcasts/MLHub/blob/master/khhoseynpour.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "یک روش جدید برای تشخیص نفوذ در شبکه‌های اینترنت اشیاء (IoT) با استفاده از *شبکه‌های عصبی کانولوشنی (CNN)* و *بهینه‌سازی ازدحام ذرات (APSO)* می‌پردازد. هدف این تحقیق شناسایی دقیق انواع مختلف حملات نفوذ است که توسط میزبان‌های کنترل‌شده توسط حمله‌کنندگان انجام می‌شود. روش APSO-CNN با بهینه‌سازی پارامترهای ساختاری CNN و معرفی یک معیار ارزیابی جدید، عملکرد بهتری نسبت به روش‌های سنتی مانند R-CNN، SVM و FNN نشان می‌دهد. نتایج نشان می‌دهد که APSO-CNN دقت شناسایی بالایی دارد و نرخ هشدار کاذب را کاهش می‌دهد، که این رویکرد را به ابزاری مؤثر و قابل اعتماد برای امنیت شبکه‌های IoT تبدیل می‌کند.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCKgnPbxHMMg",
        "outputId": "44156a66-8dd5-4abd-970d-b8527b9a55ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyswarm\n",
            "  Downloading pyswarm-0.6.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyswarm) (1.26.4)\n",
            "Building wheels for collected packages: pyswarm\n",
            "  Building wheel for pyswarm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyswarm: filename=pyswarm-0.6-py3-none-any.whl size=4464 sha256=d4969acfe641312b44ee11e3cf0b5f592c5fa9669d190677f9bbf47fb5d49ab9\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/4f/ec/8970b83323e16aa95034da175454843947376614d6d5e9627f\n",
            "Successfully built pyswarm\n",
            "Installing collected packages: pyswarm\n",
            "Successfully installed pyswarm-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pyswarm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8tO5Hcu4v5A",
        "outputId": "6c51851e-ac3d-477e-e8f0-ba0d4f4c6326"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "WARNING:tensorflow:5 out of the last 16 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x7d850ff43600> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 21 calls to <function TensorFlowTrainer._make_function.<locals>.multi_step_on_iterator at 0x7d84b0164680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from pyswarm import pso\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# تنظیمات اولیه\n",
        "num_classes = 8  # تعداد کلاس‌های حملات شبکه\n",
        "input_shape = (23, 5, 1)  # ویژگی‌های ورودی\n",
        "\n",
        "# داده‌های آموزش و اعتبارسنجی (کاهش اندازه داده‌ها)\n",
        "x_train = np.random.random((20, 23, 5, 1))  # داده‌های آموزشی با 20 نمونه\n",
        "y_train = np.random.randint(0, num_classes, 20)  # برچسب‌های آموزشی\n",
        "y_train = to_categorical(y_train, num_classes)  # تبدیل برچسب‌ها به قالب One-hot\n",
        "\n",
        "x_val = np.random.random((5, 23, 5, 1))  # داده‌های اعتبارسنجی با 5 نمونه\n",
        "y_val = np.random.randint(0, num_classes, 5)  # برچسب‌های اعتبارسنجی\n",
        "y_val = to_categorical(y_val, num_classes)  # تبدیل برچسب‌ها به قالب One-hot\n",
        "\n",
        "# تابع تعریف مدل CNN\n",
        "def create_model(params):\n",
        "    filters, kernel_size, dropout_rate, f2_neurons, f3_neurons, batch_size, learning_rate = params\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Conv2D(filters=int(filters), kernel_size=(int(kernel_size), int(kernel_size)), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(int(f2_neurons / 4), activation='relu'),  # کاهش نورون‌ها\n",
        "        layers.Dropout(float(dropout_rate)),\n",
        "        layers.Dense(int(f3_neurons / 4), activation='tanh'),  # کاهش نورون‌ها\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=keras.optimizers.SGD(learning_rate=float(learning_rate)),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# تابع ارزیابی برای بهینه‌سازی APSO\n",
        "def fitness_function(params):\n",
        "    model = create_model(params)\n",
        "    # استفاده از EarlyStopping برای کاهش زمان آموزش\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=1, verbose=0)\n",
        "    history = model.fit(x_train, y_train, batch_size=int(params[5]), epochs=5, validation_data=(x_val, y_val), verbose=0, callbacks=[early_stopping])\n",
        "    return history.history['val_loss'][-1]  # مقدار تابع خطا\n",
        "\n",
        "# تعیین محدوده مقادیر هایپرپارامترها\n",
        "lb = [100, 1, 0.4, 64, 64, 4, 0.01]  # محدوده‌های پایین (کاهش نورون‌ها)\n",
        "ub = [600, 5, 0.8, 256, 256, 16, 1]  # محدوده‌های بالا\n",
        "\n",
        "# اجرای الگوریتم APSO\n",
        "best_params, _ = pso(fitness_function, lb, ub)\n",
        "\n",
        "# ایجاد مدل نهایی با بهترین هایپرپارامترها\n",
        "best_model = create_model(best_params)\n",
        "\n",
        "# آموزش مدل نهایی\n",
        "history = best_model.fit(x_train, y_train, batch_size=int(best_params[5]), epochs=10, validation_data=(x_val, y_val), verbose=1)\n",
        "\n",
        "# ارزیابی مدل نهایی\n",
        "test_loss, test_acc = best_model.evaluate(x_val, y_val)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_acc}\")\n",
        "\n",
        "# ذخیره مدل نهایی\n",
        "best_model.save(\"final_model.h5\")\n",
        "\n",
        "# نمایش نتایج آموزش\n",
        "# ترسیم دقت و خطا\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# دقت\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# خطا\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
