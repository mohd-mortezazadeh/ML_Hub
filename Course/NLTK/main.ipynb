{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing Steps \n",
    "\n",
    "\n",
    "### A. Basic Techniques\n",
    "\n",
    "\n",
    "#### 1. Lowering Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: What is the STEP by step guide to invest In share market in india?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Lowered Sentence: what is the step by step guide to invest in share market in india?\n"
     ]
    }
   ],
   "source": [
    "sentence=\"What is the STEP by step guide to invest In share market in india?\"\n",
    "sentence_lower=str(sentence).lower()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Lowered Sentence:\", sentence_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Removing Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punc=string.punctuation\n",
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Ques\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Sentence without Punctuations: Hello Everyone, this is team Data Dynamos We are got an project of Quora Ques\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hello Everyone, this is team Data Dynamos ! We are got an project of Quora Ques\"\n",
    "without_punc=[word for word in sentence.split(\" \") if word not in list(punc)]\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without Punctuations:\", \" \".join(without_punc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing Special Characters and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Find the remainder when  math          math  is divided by       \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentence=\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\"\n",
    "sentence_clean=re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", sentence_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Removal of HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: <h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hello Guys How Are You\n"
     ]
    }
   ],
   "source": [
    "sentence='''<h3 style=\"color:red; font-family:Arial Black\">Hello Guys How Are You</h3>'''\n",
    "clean_sentence=re.sub(\"<.*?>\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Removing URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Senti\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: I visited \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "/tmp/ipykernel_5478/1469561357.py:2: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  clean_sentence=re.sub(\"(http|https|www)\\S+\", \"\", sentence)\n"
     ]
    }
   ],
   "source": [
    "sentence=\"I visited https://github.com/surajh8596/NLP-Sentiment-Analysis-/tree/main/Senti\"\n",
    "clean_sentence=re.sub(\"(http|https|www)\\S+\", \"\", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Removing Extra Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Hi    Team     Data Dynamos,  How is your                           project going on                       ?\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clean Sentence: Hi Team Data Dynamos, How is your project going on ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_5478/3963033909.py:2: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  clean_sentence=re.sub(\"\\s+\",\" \", sentence)\n"
     ]
    }
   ],
   "source": [
    "sentence=\"Hi    Team     Data Dynamos,  How is your                           project going on                       ?\"\n",
    "clean_sentence=re.sub(\"\\s+\",\" \", sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clean Sentence:\", clean_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Expanding Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m699.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m453.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "! pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We've reached final step of our data science internship. We'll meet u in projec\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Clear Sentence: We have reached final step of our data science internship. We will meet you in projec\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "sentence=\"We've reached final step of our data science internship. We'll meet u in projec\"\n",
    "clear_sentence=contractions.fix(sentence)\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Clear Sentence:\", clear_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Text Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: nltk>=3.9 in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk>=3.9->textblob) (4.67.0)\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.3/624.3 kB\u001b[0m \u001b[31m583.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.19.0\n"
     ]
    }
   ],
   "source": [
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Correct Sentence: He have reached final step of our data science Training. He'll meet you in project presentation.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "sentence=\"We have reachedd final step of our data science Trainig. We'll meet youu in project presentatiom.\"\n",
    "textblob=TextBlob(sentence)\n",
    "correct_sentence=textblob.correct()\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Correct Sentence:\", correct_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Advanced Techniques\n",
    "\n",
    "#### 1. Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/pydevcasts/Templates/MLHub/venv/lib/python3.12/site-packages (from nltk) (4.67.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pydevcasts/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punkt tokenizer is installed.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "try:\n",
    "    nltk.data.find(\"/home/pydevcasts/nltk_data/tokenizers/punkt\")\n",
    "    print(\"Punkt tokenizer is installed.\")\n",
    "except LookupError:\n",
    "    print(\"Punkt tokenizer is NOT installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "sentence =\"\"\"Our Team name is Team Data Dynamos and we have selected\n",
    "Quora question similarity project. We have started working on this project\n",
    "from 13th of May only. Working with team gives little extra space to apply\n",
    "new things.\"\"\"\n",
    "\n",
    "word_tokenize(sentence)\n",
    "# tokens=sent_tokenize(sentence)\n",
    "# print(\"Original Sentence:\", sentence)\n",
    "# print(\"--\"*60)\n",
    "# print(\"Sentence Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected Quora question simila\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Word Tokens: ['Our', 'Team', 'name', 'is', 'Team', 'Data', 'Dynamos', 'and', 'we', 'have', 'selected', 'Quora', 'question', 'simila']\n"
     ]
    }
   ],
   "source": [
    "sentence='''Our Team name is Team Data Dynamos and we have selected Quora question simila'''\n",
    "tokens=sentence.split(\" \")\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Word Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Sub-Word(n-gram character) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: Our Team name is Team Data Dynamos and we have selected\n",
      "Quora question similarity project. We have started working on this project\n",
      "from 13th of May only. Working with team gives little extra space to apply\n",
      "new things.\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "N-gram Tokens: [('Our', 'Team', 'name'), ('Team', 'name', 'is'), ('name', 'is', 'Team'), ('is', 'Team', 'Data'), ('Team', 'Data', 'Dynamos'), ('Data', 'Dynamos', 'and'), ('Dynamos', 'and', 'we'), ('and', 'we', 'have'), ('we', 'have', 'selected\\nQuora'), ('have', 'selected\\nQuora', 'question'), ('selected\\nQuora', 'question', 'similarity'), ('question', 'similarity', 'project.'), ('similarity', 'project.', 'We'), ('project.', 'We', 'have'), ('We', 'have', 'started'), ('have', 'started', 'working'), ('started', 'working', 'on'), ('working', 'on', 'this'), ('on', 'this', 'project\\nfrom'), ('this', 'project\\nfrom', '13th'), ('project\\nfrom', '13th', 'of'), ('13th', 'of', 'May'), ('of', 'May', 'only.'), ('May', 'only.', 'Working'), ('only.', 'Working', 'with'), ('Working', 'with', 'team'), ('with', 'team', 'gives'), ('team', 'gives', 'little'), ('gives', 'little', 'extra'), ('little', 'extra', 'space'), ('extra', 'space', 'to'), ('space', 'to', 'apply\\nnew'), ('to', 'apply\\nnew', 'things.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence='''Our Team name is Team Data Dynamos and we have selected\n",
    "Quora question similarity project. We have started working on this project\n",
    "from 13th of May only. Working with team gives little extra space to apply\n",
    "new things.'''\n",
    "n_gram_tokens=list(ngrams((sentence.split(\" \")), n=3))\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"N-gram Tokens:\", n_gram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en=stopwords.words(\"english\")\n",
    "print(\"Total Stop Words in English=\", len(stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Our Team name is Team Data Dynamos and we have selected Quora question similari\"\n",
    "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_en]\n",
    "print(\"Sentence with StopWOrds:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without StopWOrds:\", \" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Driving Driven Drives Able Ables Enable Enables Enabling\"\n",
    "sentence_non_stopword=[word for word in sentence.split(\" \") if not word in stopwords_en]\n",
    "print(\"Sentence with StopWOrds:\", sentence)\n",
    "print(\"--\"*60)\n",
    "print(\"Sentence without StopWOrds:\", \" \".join(sentence_non_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\"))  # خروجی: \"running\"\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # خروجی: \"good\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
